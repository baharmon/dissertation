\documentclass{article}
\usepackage{graphicx}
\graphicspath{{./images/}}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{paralist}
\usepackage[round]{natbib}
\usepackage{sectsty}
\usepackage{gensymb}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}
\usepackage[space]{grffile}
\usepackage{latexsym}
\usepackage{amsfonts,amsmath,amssymb}
\usepackage{url}
%\usepackage{minitoc}
\hypersetup{colorlinks=false,pdfborder={0 0 0}}
\usepackage{textcomp}
\usepackage{longtable}
\usepackage{multirow,booktabs}
\newcommand{\truncateit}[1]{\truncate{0.8\textwidth}{#1}}
\newcommand{\scititle}[1]{\title[\truncateit{#1}]{#1}} 
\usepackage[parfill]{parskip}

% Type
aaaaface
\usepackage{ifxetex}
\ifxetex
  \usepackage{fontspec}
  \defaultfontfeatures{Ligatures=TeX} % To support LaTeX quoting style
  \setmainfont[Mapping=tex-text, Color=textcolor]{HelveticaNeue}
  %\setmainfont[Mapping=tex-text, Color=textcolor]{Avenir LT Std}
\else
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \renewcommand{\familydefault}{\sfdefault}
  \usepackage{helvet}
\fi
%\chapterfont{\Large} % \sffamily
%\renewcommand{\chaptername}{}
%\renewcommand{\thechapter}{}

% Title page
\usepackage{xcolor}
\definecolor{titlepagecolor}{cmyk}{0,0,0,0}
\definecolor{namecolor}{cmyk}{0,0,0,1} 
\definecolor{chaptertitlepagecolor}{cmyk}{0,0,0,0.9}
\definecolor{chapternamecolor}{cmyk}{0,0,0,0.3}  

\begin{document}

%---------------------------------------------- BODY ----------------------------------------------

\section{Point clouds}
% data structure
Point clouds are data structures that represent multidimensional space as an array of coordinates. 
%
3-dimensional (3D) point clouds represent 3D space as an array of x, y, and z coordinates. 
%
More coordinates can add to point clouds to represent other dimensions of data such as time, color, or value. 
%
3D point clouds can be generated algorithmically or captured by sensing technologies like lidar, structured light, time-of-flight, or structure from motion. 
%
Point clouds captured by these sensing technologies represent sampling points on the sensed surface. 
%
% applications
Point clouds are used in diverse applications including
surveying,
spatial modeling, 
digital fabrication, robotics, and
human-computer interaction. 
%
Point clouds are used often in tangible interfaces, 
a type of human-computer interaction that aims to physically manifest digital data, 
to digitally model physical objects, coupling them with their digital counterparts. 

% point clouds in TL
Tangible Landscape -- a tangible interface for geographic information systems (GIS) -- 
uses a 3D sensor to capture a physical model of a landscape as a point cloud and import it into GIS.
%
Tangible Landscape's predecessor, the Tangible Geospatial Modeling System, used a terrestrial lidar scanner \citep{Tateosian2010},
the first generation of Tangible Landscape used the first generation Kinect with structured light sensing \citep{Petrasova2014}, 
and the second generation of Tangible Landscape used the second generation Kinect with time-of-flight sensing \citep{Petrasova2015}. 
%
Tangible Landscape couples the physical model with a digital model through a near real-time cycle of 
3D scanning, point cloud processing, geospatial modeling and simulation, and projection. 
%
This enables users to tangibly interact with digital models and simulations
either by shaping topography with their hands or 
by placing markers that are identified through object detection. 
%
As the digital models and simulations update
the results are projected back onto the model for the user to see. 
%
Tangible Landscape also uses 3D data derived from airborne lidar point clouds as reference data. 
%
In either case the point clouds are converted into raster data for geospatial modeling, analysis, and simulation.


\subsection{Acquisition}
%
\paragraph{Lidar}	
In lidar sensing 
the time a pulse of light takes to travel from the sensor and reflect back 
is used to measure 3D space.
%
A laser, a pulse of light is emitted from the sensor, 
travels until it hits an object and is reflected back to the sensor. 
The distance travelled by the pulse is computed from its travel time. 
The position of the scanned object is computed 
from the distance travelled by the pulse
and
the position and orientation of the sensor. 
%
The position of the scanned object is recorded as a 3D point. 
A lidar survey generates a set of 3D points -- a point cloud. 

% terrestrial lidar
Terrestrial lidar can be used to scan small objects or survey landscapes. 
Turntables are often used to precisely 3D scan all around small objects and capture all of the geometry. 
Terrestrial lidar surveys can precisely capture features in a landscape at centimeter to millimeter resolution -- 
% for example ...
\citeauthor{Starek2013}, for example, surveyed gullies at 1 cm resolution from 12 m range \citeyearpar{Starek2013}. 
%
Terrestrial surveys of entire landscapes, however, tend to have highly spatially variable densities
with sparse and dense regions
due to obstacles 
and the dispersion of points throughout the field of view.

% airborne lidar
Entire landscapes can be precisely 3D scanned as point clouds with airborne lidar at meter resolution.
%
As pulses of light are emitted from the aircraft they hit and reflect back from vegetation, structures, and topography.
When a pulse of light hits a tree part of the light is reflected 
and recorded as the first return, while the rest penetrates the outer canopy. 
The residual pulse, recorded as a series of returns, reflects off of leaves, branches, 
shrubs, and the ground. 
The waveform of the pulse can also be recorded to model the structure of the canopy.

\paragraph{Structured light}
Structured light -- the deformation of a known pattern of light over a surface -- can be used to reconstructed 3D depth. 
%
Structured light depth sensors like the first generation Kinect 
project a known pattern of infrared (IR) light onto a scene or object 
while scanning the scene with an IR sensor
to capture the deformation of the pattern of IR light. 
The depth of the scene can be interpolated from the deformation of the pattern
\citep{Hansard2013}. 
%
% kinect
The first generation Kinect combines an IR projector, an IR sensor, and a color sensor 
to generate IR amplitude, depth, and color images \citep{Smisek2011,Hansard2013}. 
The maximum resolution of the depth image is 640×480 pixels. 
%
The first generation Kinect can capture depth images
at 0.8 - 3.5 m range 
with 3 mm xy resolution and 10 mm z resolution at a 2 m distance. 
%
% errors
Structured light depth sensing suffers from data loss -- holes in the reconstruction -- 
at the boundary of objects and with transparent or specular materials \citep{Hansard2013}. 

\paragraph{Time-of-flight}
In time-of-flight depth sensing
the phase delay between projected and reflected IR light 
is used compute depth \citep{Hansard2013}. 
%
% kinect
The second generation Kinect uses a time-of-flight depth sensor 
with a range of 0.8 - 4.2 m, a maximum depth resolution of 512 × 424 pixels, and a frame rate of 30 frames per second \citep{Bamji2015}. 
%
% errors
Time-of-flight depth sensing suffers from data drift -- offsets in depth values -- 
at the boundary of objects and with transparent materials 
and from and data loss with specular materials.
%
The geometry, color, and material of the scene or object cause 
variations in the phase and amplitude of reflected IR light giving rise to errors in time-of-flight sensing. 
%
Systematic errors include noise caused by short integration times
and ambiguity caused by variations in amplitude due to color. 
%
Non-systematic errors include motion blur 
and scattering caused by IR saturation, ambiguous borders, and overlapping reflected IR signals 
\citep{Hansard2013}. 

\paragraph{Structure from motion}
In structure from motion 
3D surfaces are algorithmically reconstructed 
by matching features captured from multiple viewpoints in overlapping, offset images. 
%
Image matching algorithms match features in random, unstructured sets of photographs 
with enough overlapping images.
Bundle adjustment is then used to optimize the camera parameters and the geometry of the surface 
\citep{Westoby2012, Fonstad2013,Snavely2008}. 

% aerial / uav
Structure from motion can be used to reconstruct DEMs from aerial photography 
captured by manned or unmanned aerial vehicles (UAVs). 
%
Structure from motion for aerial photography generates a 3D point cloud from matched features 
that can be georeferenced using ground control points or the recorded position of the camera \citep{Fonstad2013}.

% resolution
The resolution of the point cloud depends upon 
the height of the aircraft, the resolution of the camera, and the percent overlap of the photographs. 
Imagery from low altitude aircraft like UAVs can be used to generate high resolution point clouds -- 
for example
imagery captured by a UAV flying at an altitude of 120 m 
with a 2456 x 1632 pixel resolution camera 
can generate a 10 cm resolution point cloud. 
% 118.756 m altitude, 15cm / 6cm resolution

\subsection{Modeling}
% modeling digital surfaces from point clouds
Once a point cloud has been captured 
it can be used to generate other 3D representations 
such as meshes, parametric surfaces and volumes, 
2.5D rasters, and 3D rasters.
2.5D rasters are xy arrays of cells with a z value representing elevation,
whereas 3D rasters are xyz arrays of volumetric cells 
that can have a value representing a 4th dimension of data. 
%

% DEMs / DSMs
Raster representations of landscapes such as 
digital elevation models (DEMs)
-- rasters representing bare earth topography --
and digital surface models (DSMs)
-- rasters representing topography, vegetation, and structures -- 
are commonly used in GIS to model and analyze terrain because
they can easily be mathematically transformed. 
%
The partial derivatives of a function approximating a DEM or DSM, for example, 
can be used to compute topographic parameters such as slope, aspect, and curvature. 

% point clouds from 3D scanning
Point clouds representing landscapes captured by 3D sensing technologies like
structure from motion and time-of-flight require processing
before they can be modeled as DEMs or DSMs. 
%
Outlying points should be filtered and the data should be georeferenced. 
%
Sometimes edge detection can be used to identify boundaries and extract a region. 

% filtering
Lidar point clouds stored in the las format can be filtered by returns and, if classified, by classes. 
%
They can be filtered by the first return, middle return, or last return.
They can also be filtered by the following classes:
0 for created,1 for unclassified, 2 for ground, 3 for low vegetation
4 for medium vegetation, 
5 for high vegetation,
6 for building,
7 for low point, 
8 for model key-point, 
9 for water,
10 for reserved, 
11 for reserved.
12 for overlap, 
and 13-31 for reserved 
\citep{ASPRS2013}.

% filtering unclassified data
Unclassified point clouds -- whether from lidar or other 3D sensing technologies like
structure from motion and time-of-flight -- can be automatically classified. 
%
Edge detection can be used to identify and classify features like buildings. 
%
Ground points can be automatically classified using the 
multi-scale curvature classification algorithm, 
`an iterative multiscale algorithm for classifying LiDAR returns
that exceed positive surface curvature thresholds'
\citep{Evans2007}. 

% filtering
Point clouds should be filtered before being used to generate DEMs or DSMs. 
%
Only points classified as ground should be used to model bare earth topography as a DEM. 
%
Only the first return and classes 1,2,3,4,5,6,9 should be used to 
model topography, vegetation, and buildings as a DSM. 

Raster surfaces like DEMs and DSMs can be computed from point clouds 
using either binning or interpolation.
%
% binning
In binning values are assigned to cells based on 
a univariate statistical analysis of the z coordinates of the set of points 
contained in the cell. 
Statistical methods for computing raster values from a set of points include 
sample size (n), minimum, maximum, range, sum, mean, standard deviation, variance, the coefficient of variance, median, and percentile. 
Binning is a fast method, but can result in rough surfaces with holes in regions without enough points. 

% interpolation
In interpolation the values between points are estimated mathematically or statistically. 
%
Interpolation can be processing intensive, 
but can fill holes in data and 
-- with the right choice of method and parameters  -- 
can smoothly, precisely model complex surfaces.
%
Poorly parameterized methods can cause distortions like  
overfitting (with overshooting and pitting) or over smoothing. 
%
Methods include 
kriging, 
radial basis function,
natural neighbor,
inverse weighted distance,
bicubic spline,  
and regularized spline with tension
interpolation.
%


%---------------------------------------------- BIBLIOGRAPHY ----------------------------------------------

\bibliographystyle{plainnat}
\bibliography{../tangible_topography} 
\end{document}








